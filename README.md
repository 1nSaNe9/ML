# Лабораторная работа 3. Линейная регрессия
## Задание
Перед выполнением лабораторной работы необходимо загрузить набор данных в соответствии с вариантом на диск

1. Написать программу, которая разделяет исходную выборку на обучающую и тестовую (training set, test set). Использовать стандартные функции (train_test_split и др. нельзя).
2. С использованием библиотеки scikit-learn обучить модель линейной регрессии по обучающей выборке пример
3. Проверить точность модели по тестовой выборке
4. Построить модель с использованием полиномиальной функции. Построить графики зависимости точности на обучающей и тестовой выборке от степени полиномиальной функции.
5. Построить модель с использованием регуляризации. На основе экспериментов подобрать параметры для регуляризации. Построить графики зависимости точности модели на обучающей и тестовой выборках от коэффициента регуляризации.
## Вариант 6
Concrete Compressive Strength

## Загрузка dataset
```
train_set = pd.read_csv('trainingData.csv')
test_set = pd.read_csv('validationData.csv')
```
X - Features, Признаки (сигналы Wi-Fi точек доступа). y - Targets, Цели (координаты LONGITUDE/LATITUDE)

В данной работе используется набор данных по Wi-Fi позиционированию в помещениях. Для уменьшения вычислительной сложности рассматриваются полиномы степени 1-2, а также применяется уменьшение размерности данных до 50 наиболее значимых признаков.

## 1. 1. Разделение исходной выборки на обучающую и тестовую (80/20)
В данной работе используются предварительно разделенные данные:

trainingData.csv - обучающая выборка

validationData.csv - тестовая выборка

Дополнительно выполняется предобработка данных:

Преобразование всех колонок в числовой формат

Удаление строк с пропущенными значениями (NaN)

Выбор целевой переменной (LONGITUDE, LATITUDE, FLOOR или BUILDINGID)
```
exclude_columns = [target_column, 'LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID',
                   'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP']

y_data = data[target_column]
x_data = data.drop(columns=[col for col in exclude_columns if col in data.columns])

# Получаем общее количество наблюдений
n_samples = x_data.shape[0]
print(f"Общее количество наблюдений: {n_samples}")

# Создаем массив индексов и перемешиваем их
indices = np.arange(n_samples)
np.random.seed(42)  # Для воспроизводимости результатов
np.random.shuffle(indices)

# Применяем перемешанные индексы к данным
x_shuffled = x_data.iloc[indices]
y_shuffled = y_data.iloc[indices]

# Разделяем на обучающую (80%) и тестовую (20%) выборки
train_size = int(n_samples * 0.8)

x_train_set = x_shuffled[:train_size]
x_test_set = x_shuffled[train_size:]

y_train_set = y_shuffled[:train_size]
y_test_set = y_shuffled[train_size:]

```
## 2. Модель линейной регрессии
Линейная регрессия - статистический метод моделирования зависимости между независимыми переменными (признаками) и зависимой переменной (целевой величиной).

Для уменьшения вычислительной сложности и проблем с памятью применяется уменьшение размерности до 50 наиболее значимых признаков с помощью SelectKBest.

R² (коэффициент детерминации) – это нормированная метрика, показывающая, какую долю изменчивости целевой переменной модель смогла объяснить

R² показывает:

1.0 - идеальное предсказание 

0.0 - модель не лучше среднего значения

< 0 - модель хуже простого среднего

```
# Уменьшение размерности
selector = SelectKBest(score_func=f_regression, k=min(50, x_train_set.shape[1]))
x_train_reduced = selector.fit_transform(x_train_set, y_train_set)
x_test_reduced = selector.transform(x_test_set)

# Линейная регрессия
model = LinearRegression()
model.fit(x_train_reduced, y_train_set)
```



## 3. Проверить точность модели по тестовой выборке
Проверка точности - это процесс оценки способности модели делать правильные предсказания на новых, ранее не виденных данных.

Для оценки качества модели используются метрики. Они помогают понять, насколько хорошо модель справляется с задачей предсказания. Мы можем использовать две основные метрики: MSE и R².

MSE (среднеквадратичная ошибка) измеряет абсолютное среднее отклонение предсказаний от реальных значений. Она удобна для сравнения моделей, работающих с одними единицами измерения, но ее значение сильно зависит от масштаба данных и чувствительно к выбросам.

R² (коэффициент детерминации) – это нормированная метрика, показывающая, какую долю изменчивости целевой переменной модель смогла объяснить. R² всегда находится в диапазоне от минус бесконечности до 1, где 1 означает идеальное предсказание. Благодаря своей нормированности, R² позволяет сравнивать модели независимо от масштаба данных, что делает его универсальным инструментом оценки.

Остановимся на использовании метрики R2. 

```
r2_train = r2_score(y_train_set, y_train_pred)
r2_test = r2_score(y_test_set, y_test_pred)

mse_train = mean_squared_error(y_train_set, y_train_pred)
mse_test = mean_squared_error(y_test_set, y_test_pred)
```

## 4.Построить модель с использованием полиномиальной функции
Полиномиальная регрессия позволяет моделировать нелинейные зависимости между сигналами Wi-Fi точек и координатами. В данной работе рассматриваются степени 1 и 2 для избежания проблем с памятью и переобучением.

Создается pipeline, который автоматически преобразует признаки в полиномиальные и применяет линейную регрессию.

Полиномиальная регрессия — это метод машинного обучения, используемый для моделирования нелинейных зависимостей между переменными путем аппроксимации данных полиномом степени (k). В отличие от линейной регрессии, которая предполагает прямую связь, этот метод позволяет учитывать более сложные криволинейные тренды в данных, что делает его полезным в различных областях, таких как экономика и инженерия.
```
degrees = [1, 2]  # Убираем высшие степени чтобы избежать проблем с памятью

for degree in degrees:
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline([
        ("polynomial_features", polynomial_features),
        ("linear_regression", linear_regression),
    ])
```
Строится график зависимости точности модели от степени полинома, что позволяет визуально определить оптимальную степень.

## 5. Построить модель с использованием регуляризации
Ridge-регрессия (или гребневая регрессия) — это регуляризованная версия линейной регрессии, которая используется для борьбы с мультиколлинеарностью (линейной зависимостью между предикторами) и переобучением. Она добавляет к функции потерь штрафное слагаемое в виде квадрата коэффициентов, чтобы сделать веса модели меньше, но не обнулить их, в отличие от Lasso-регрессии.

Ridge-регуляризация (L2) применяется для борьбы с переобучением, которое может возникнуть при использовании полиномиальных признаков. Ridge добавляет штрафное слагаемое в виде квадрата коэффициентов к функции потерь.

Алгоритм обучения пытается найти баланс между подгонкой под данные и поддержанием коэффициентов на низком уровне.
```
pipeline = Pipeline([
    ("poly", PolynomialFeatures(degree=optimal_degree, include_bias=False)),
    ("scaler", StandardScaler()),
    ("ridge", Ridge(alpha=a, max_iter=1000))
])
```
Параметр α (alpha) контролирует силу регуляризации. В работе используется логарифмическая шкала от 10⁻⁴ до 10³ для поиска оптимального значения. Строится график зависимости точности от коэффициента регуляризации.

Для борьбы с переобучением и коррелированными признаками применим регуляризацию Ridge (L2). Она добавляет штраф к сумме квадратов коэффициентов модели, что стабилизирует обучение и уменьшает влияние сильно коррелированных признаков.

Почему я использовал Ridge? 
1. Полиномиальные признаки сильно коррелированы между собой
2. Ridge лучше справляется с мультиколлинеарностью
3. Сохраняет все признаки


